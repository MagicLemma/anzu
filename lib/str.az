
fn equal(lhs: char const[], rhs: char const[]) -> bool
{
    if len(lhs) != len(rhs) { return false; }
    var idx := 0u;
    while idx < len(lhs) {
        if lhs[idx] != rhs[idx] { return false; }
        idx = idx + 1u;
    }
    return true;
}

fn find(string: char const[], substr: char const[], start: u64) -> u64
{
    var idx := start;
    while idx < len(string) - len(substr) {
        let curr_substr := string[idx : idx + len(substr)];
        if equal(substr, curr_substr) {
            return idx;
        }
        idx = idx + 1u;
    }
    return len(string);
}

fn contains(string: char const[], substr: char const[]) -> bool
{
    return find(string, substr, 0u) != len(string);
}

fn replace(arr: arena&, string: char const[], from: char const[], to: char const[]) -> char const[]
{
    return nullptr;
}

struct tokenizer
{
    _string: char const[];
    _delim: char;
    _start: u64;
    _curr: u64;

    fn advance(self: &) -> null
    {
        # we are at the last word, so "advance" just invalidates the tokenizer
        if self._curr == len(self._string) {
            self._start = self._curr;
            return;
        }

        # if we are not at the first word, we need to update the start location
        if self._curr > 0u {
            self._curr = self._curr + 1u;
            self._start = self._curr;
        }

        while self._curr < len(self._string) && self._string[self._curr] != self._delim {
            self._curr = self._curr + 1u;
        }
    }

    fn valid(self: const&) -> bool
    {
        return self._start != len(self._string);
    }

    fn current(self: const&) -> char const[]
    {
        return self._string[self._start : self._curr];
    }

    fn create(input: char const[], delim: char) -> tokenizer
    {
        var t := tokenizer(input, delim, 0u, 0u);
        t.advance(); # prime the tokenizer at the first word
        return t;
    }
}