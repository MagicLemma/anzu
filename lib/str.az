
fn equal(lhs: char const[], rhs: char const[]) -> bool
{
    if len(lhs) != len(rhs) { return false; }
    var idx := 0u;
    while idx < len(lhs) {
        if lhs[idx] != rhs[idx] { return false; }
        idx = idx + 1u;
    }
    return true;
}

fn find(string: char const[], substr: char const[], start: u64) -> u64
{
    var idx := start;
    while idx <= len(string) - len(substr) {
        let curr_substr := string[idx : idx + len(substr)];
        if equal(substr, curr_substr) {
            return idx;
        }
        idx = idx + 1u;
    }
    return len(string);
}

fn contains(string: char const[], substr: char const[]) -> bool
{
    return find(string, substr, 0u) != len(string);
}

fn occurrences(string: char const[], substr: char const[]) -> u64
{
    var count := 0u;
    var idx := 0u;
    while idx < len(string) {
        let curr_substr := string[idx : idx + len(substr)];
        if equal(substr, curr_substr) {
            count = count + 1u;
            idx = idx + len(substr);
        } else {
            idx = idx + 1u;
        }
    }
    return count;
}

fn copy(dst: char[], src: char const[])
{
    assert len(dst) >= len(src);
    var idx := 0u;
    while idx < len(src) {
        dst[idx] = src[idx];
        idx = idx + 1u;
    }
}

fn replace(arr: arena&, string: char const[], from: char const[], to: char const[]) -> char const[]
{
    let new_size := len(from) == len(to) ? len(string)
                                         : len(string) + occurrences(string, from) * (len(to) - len(from));

    var new_string := new(arr, new_size) ' ';
    var idx := 0u;
    while idx < len(string) {
        let next := find(string, from, idx);
        copy(new_string[idx : next], string[idx : next]);
        idx = next;
        if next != len(string) {
            copy(new_string[idx : idx + len(to)], to);
            idx = idx + len(to);
        }
    }
    return new_string;
}

struct tokenizer
{
    _string: char const[];
    _delim: char;
    _start: u64;
    _curr: u64;

    fn advance(self: &) -> null
    {
        # we are at the last word, so "advance" just invalidates the tokenizer
        if self._curr == len(self._string) {
            self._start = self._curr;
            return;
        }

        # if we are not at the first word, we need to update the start location
        if self._curr > 0u {
            self._curr = self._curr + 1u;
            self._start = self._curr;
        }

        while self._curr < len(self._string) && self._string[self._curr] != self._delim {
            self._curr = self._curr + 1u;
        }
    }

    fn valid(self: const&) -> bool
    {
        return self._start != len(self._string);
    }

    fn current(self: const&) -> char const[]
    {
        return self._string[self._start : self._curr];
    }

    fn create(input: char const[], delim: char) -> tokenizer
    {
        var t := tokenizer(input, delim, 0u, 0u);
        t.advance(); # prime the tokenizer at the first word
        return t;
    }
}